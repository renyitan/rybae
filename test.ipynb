{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to gpt2 (https://huggingface.co/gpt2)\n"
     ]
    }
   ],
   "source": [
    "pipe = transformers.pipeline('text-generation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "\n",
    "def clean_text(txt):\n",
    "    return ' '.join(txt.strip().split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'what is the meaning of life?\\n\\nWhat would an understanding of life become when we become aware of it?\\n\\nHow does life be understood when we cease to be aware of it?\\n\\nHow does life become understood when we cease to'}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "prompt=\"what is the meaning of life?\"\n",
    "output = pipe(prompt)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['What would an understanding of life become when we become aware of it? How does life be understood when we cease to be aware of it? How does life become understood when we cease to']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "responses = list(map(lambda x: clean_text(\n",
    "    x['generated_text'][len(prompt):]), output))\n",
    "responses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def build_ranker_dict():\n",
    "    \"\"\"Build dictionary of ranker weights and pipelines.\"\"\"\n",
    "\n",
    "    human_vs_rand_weight = None\n",
    "    human_vs_machine_weight = None\n",
    "    updown_weight = None\n",
    "    depth_weight = None\n",
    "    width_weight = None\n",
    "\n",
    "    ranker_dict = dict()\n",
    "    if human_vs_rand_weight is not None:\n",
    "        ranker_dict['human_vs_rand'] = dict(\n",
    "            pipe=transformers.pipeline(\n",
    "                'sentiment-analysis', model='microsoft/DialogRPT-human-vs-rand'),\n",
    "            weight=human_vs_rand_weight,\n",
    "            group='prior'\n",
    "        )\n",
    "    if human_vs_machine_weight is not None:\n",
    "        ranker_dict['human_vs_machine'] = dict(\n",
    "            pipe=transformers.pipeline(\n",
    "                'sentiment-analysis', model='microsoft/DialogRPT-human-vs-machine'),\n",
    "            weight=human_vs_machine_weight,\n",
    "            group='prior'\n",
    "        )\n",
    "    if updown_weight is not None:\n",
    "        ranker_dict['updown'] = dict(\n",
    "            pipe=transformers.pipeline(\n",
    "                'sentiment-analysis', model='microsoft/DialogRPT-updown'),\n",
    "            weight=updown_weight,\n",
    "            group='cond'\n",
    "        )\n",
    "    if depth_weight is not None:\n",
    "        ranker_dict['depth'] = dict(\n",
    "            pipe=transformers.pipeline(\n",
    "                'sentiment-analysis', model='microsoft/DialogRPT-depth'),\n",
    "            weight=depth_weight,\n",
    "            group='cond'\n",
    "        )\n",
    "    if width_weight is not None:\n",
    "        ranker_dict['width'] = dict(\n",
    "            pipe=transformers.pipeline(\n",
    "                'sentiment-analysis', model='microsoft/DialogRPT-width'),\n",
    "            weight=width_weight,\n",
    "            group='cond'\n",
    "        )\n",
    "    return ranker_dict\n",
    "\n",
    "ranker_dict = build_ranker_dict()\n",
    "ranker_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What would an understanding of life become when we become aware of it? How does life be understood when we cease to be aware of it? How does life become understood when we cease to'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def pick_best_response(prompt, responses, ranker_dict):\n",
    "    if len(ranker_dict) == 0:\n",
    "        return random.choice(responses)\n",
    "\n",
    "m = pick_best_response(prompt, responses, ranker_dict)\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 2.88G/2.88G [03:27<00:00, 14.9MB/s]\n",
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at microsoft/DialoGPT-large.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelWithLMHead, TFAutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('microsoft/DialoGPT-large')\n",
    "model = TFAutoModelForCausalLM.from_pretrained('microsoft/DialoGPT-large')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4bd624a0593993fe43ac4046b27b898fb2ef75c21c08f81e89e64ea0f51df676"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('tensorflow': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
